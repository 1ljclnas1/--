# 8. 分布式系统的挑战

**做一个悲观假定：多有可能出错的事情一定会出错**

故障可能来自网络问题，以及时钟与时序问题等。讨论问题的可控程度。

## 故障与部分失效

单节点：要么故障，要么正常。bug多数是软件本身的bug

单节点的软件通常不会出现模棱两可的情况。（确定性）

这背后设计一个非常审慎的选择：如果发生了某种内部错误，宁可计算机崩溃，而不是返回一个错误结果。

对于分布式系统，可能会出现一部分机器正常，一部分出现**难以预测**的故障，这叫部分失效。而网络又带来了消息延迟的不确定性。

正是由于这几个原因，提高了分布式系统的复杂性。

## 云计算和超算

关于如何构建大规模计算系统有几个思路：

- 规模的一个极端是高性能计算（HPC）。包含成千上万个CPU的超级计算机构成一个庞大集群，通常用于计算密集型的科学计算任务，如天气预报或分子动力学（模拟原子和分子运动）

- 另一个极端是云计算。有几个特点：多租户数据中心，通用计算机，用IP以太网连接，弹性/按需资源分配，并按需计费

- 传统企业数据中心则位于以上两个极端之间。

高性能计算式定期对任务进行快照，当节点出现故障，停止整个集群，然后等待修复以后，从快照恢复。这么看的话其实HPC更像是一个单节点系统，因为它将局部失效提升为整体失效。

但是：

- 大部分互联网服务都是在线的，需要随时提供**低延迟服务**。不可能提升为整体失效。离线的批处理任务倒是可以随时停下。

- 高性能计算通常采用专用硬件，每个节点的可靠性很高，节点间主要通过共享内存或远程内存直接访问等技术进行通信。而云计算节点多是通用机器构建，出于大规模部署时经济因素考虑，单节点的成本低廉主要由依靠较高的集群聚合性能，但另一方面也具有较高的故障率。

- 大型数据中心通常基于IP和以太网，采用Clos拓扑结构提供等分带宽。高性能计算通常采用特定的网络拓扑结构，例如多为网格和trouses。

- 系统越大，其中局部组件失效的概率越大。

- 如果系统可以容忍局部失效，对运维的帮助很大。

- 对于全球分散部署的多数据中心，通信很可能经由广域网，速度慢且不可靠。

在不可靠的组件之上构建可靠系统，必须依靠软件系统提供的容错能力。

个人观点：**对于分布式来说故障是系统正常运行的一部分**

## 不可靠的网络

互联网以及大多数数据中心的内部网络都是异步网络。一个节点可以发送消息到另一个节点，但网络不保证什么时候到达，甚至以一定到达。发送之后等待响应过程中，有很多事情可能出错：

1. 请求可能已经丢失（比如有人拔掉了网线）

2. 请求可能正在某个队列中等待，无法马上发送（也许网络或接收方已经超负荷）

3. 远程接收节点可能已经失效（例如崩溃或关机）

4. 远程节点可能暂时无法响应（长时间的GC）

5. 远程节点的回复丢失

6. 远程节点回复延迟

这些问题无法区分，唯一能确定的就是尚未收到响应。

处理这个问题通常采用超时机制：等待一段时间后没收到回复则放弃，并认为响应不会到达。

## 现实中的网络故障

乱七八糟的，甚至可能有鲨鱼咬破海底电缆。增加冗余设备可能不会显著减少故障（人为错误，比如配置错误）

## 检测故障

许多系统都需要自动检测节点失效这样的功能。

- 负载均衡器需要避免向已经失效的节点继续分发请求（即将其下线）

- 对于主从复制的分布式数据库，如果主节点失败，需要将某个从节点提升，不过由于网络的不确定性很难判断是否真的失效。（对此就要进行处理）

- 假设可以登录节点，但发现服务进程没有侦听目标端口（例如进程崩溃），那么操作系统会返回RST或FIN标志的数据包来辅助关闭或拒绝TCP链接。但是，如果节点是在处理请i取得过程中发生了崩溃，则很难知道该节点实际处理了多少数据。（此时需要全部回滚，以保证一致性，但是会违反）

- 如果服务进程崩溃（或被杀死），但操作系统仍正常运行，可以通过脚本通知其他节点，以便新节点来快速接管而跳过等待超时。HBase采用了这样的方法。

- 如果有权访问数据中心网络交换机，则可以通过管理接口查询是否存在硬件级别的链路故障。不过，该方法也有局限性，例如通过互联网连接，或出于共享数据中心而没有访问交换机的权限，以及由于网络问题而根本无法登录管理界面等。

- 如果路由器已经确认目标节点不可访问，则会返回ICMP“目标不可达”数据包来回复请求。但是，路由器本身并不具有什么神奇的检测能力，从这一点来讲，它和网络上其他节点并无本质区别。

想要知道一个请求是否执行成功，就需要应用级别的回复。

## 超时与无限期的延迟
