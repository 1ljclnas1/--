引自 

[向量数据库｜一文全面了解向量数据库的基本概念、原理、算法、选型-腾讯云开发者社区-腾讯云 (tencent.com)](https://cloud.tencent.com/developer/article/2312534)

# 什么是向量数据库？

向量数据库的原理和实现，包括向量数据库的基本概念、相似性搜索算法、相似性测量算法、过滤算法和向量数据库的选型等等。

## GPT的缺陷

上下文限制。

首先GPT作为LLM是不具备记忆功能的，所谓记忆功能是开发者将对话记录存储在内存或者数据库中，当你发送消息给gpt的时候，程序会自动将最近的几次对话记录（限制在4096以内）通过prompt组合成最终的问题，并发送给GPT。简而言之记忆窗口是4096token。

像claude的限制在100k左右，但是也并不能解决问题，因为他的响应速度会变得很慢而且它是按照token收费的。

有点儿类似于早期开发者的内存昂贵阶段，这里是token昂贵阶段。一个是贵，一个是小。

## 向量数据库的崛起

向量数据库就是解决方案之一。其核心思想是将文本转换成向量，然后将向量存储在数据库中，当用户输入问题时，将问题转换成向量，然后在数据库中搜索最相似的向量和上下文，最后将文本返回给用户。

比如，当有一份文档需要GPT处理的时候，先将这个文档的内容转化为向量（vector embedding），然后用户提出的问题转换为向量，再然后在数据库中搜索最相似的向量，匹配最相似的几个上下文，最后将上下文返回给GPT。这样不仅大大减少了GPT的计算量，从而提高相应速度，更重要的是降低成本，并绕过了GPT的tokens限制。

![](C:\Users\ljc\Documents\GitHub\--\数据库\向量数据库\图片\vector_embedding.png)

再比如，我与GPT之间有一段很长的对话，可以以向量的形式将对话保存起来，当给GPT提问的时候，通过将问题转化为向量，对过去所有的聊天记录进行于一搜索，找到与当前问题最相关的记忆，一并发送给GPT，极大的提高GPT的输出质量。

向量数据库的作用不止于文字语义搜索，还包含人脸识别、图像搜索、语音识别等功能，但不可否认的是，现在向量数据库的爆火，正是因为它对于AI理解和长期记忆的作用。

## Vector Embeddings

对于传统数据库，搜索功能都是基于不同的索引方式（B Tree、倒排序等）加上精确匹配和排序算法（BM25、TF-IDF）等实现的。本质还是基于文本的精确匹配，这种索引和搜索算法对于关键字的搜索功能非常合适，但对于语义搜索功能就非常弱。

比如你搜索小狗，得不到柯基、金毛等。所以传统数据库需要把这些词之间打上关联，才能实现语义搜索。而这个将生成和挑选特征这个过程也被称为Feature Engineering(特征工程)，它是将原始数据转化成更好的表达问题本质的特征过程。

但是如果处理非结构化数据比如图片，音频，那特征数量将会迅速膨胀。很难人为的进行标注，所以需要一种自动化的方式提取特征，可以通过Vector Embedding实现。

Vector Embedding是由AI模型（比如LLM）生成的，它会根据不同的算法生成高维度的向量数据，代表着数据的不同特征，这些特征代表了数据的不同维度。例如，对于文本，这些特征可能包括词汇、语法、语义、情感、情绪、主体、上下文等。对于音频，这些特征可能包括音调、节奏、音高、音色、音量、语音、音乐等。

例如对于目前来说，文本向量可以通过OpenAI的text-embedding-ada-002模型生成，图像向量可以通过clip-vit-base-patch32模型生成，音频可以通过wav2vec2-base-960h模型生成，都是具有语义信息的。

## 特征和向量

向量数据库的核心在于相似性搜索。

那么什么是特征和向量呢？

描述一个物体的多个维度，组成的就是向量。

通过计算向量之间的距离，判断相似性，就是相似性搜索

## 相似性搜索Similarity Search

弱国对数据库中的每一个向量都做一次计算，计算量太大是不可能的。

因此需要一些高效的搜索算法，其思想主要基于两种方式：

1. 减少向量大小----通过降维减少表示向量值的长度

2. 缩小搜索范围---可以通过聚类或将向量组织成基于树形、图形结构来实现，并限制搜索范围尽在最接近的簇中进行，或者通过最相似的分支进行过滤。

大部分算法都有一个共同的概念聚类

K-Means和Faiss

在保存向量数据后，先对向量数据进行聚类。，经过聚类算法不断调整聚类中心的位置，划分为簇。每次搜索时，只需要先判断向量属于哪个簇，然后在这个簇中进行搜索，这样可以减少搜索范围。

然而传统的聚类算法可能会遗漏向量，比如相似的向量处于边界处，为了解决这个问题可以将搜索范围动态调整，动态搜索最近的n个中心，根据业务需求定。

## Product Quantization(PQ)

在大规模数据集中，聚类算法的最大问题在于内存占用太大。

- 首先，因为需要保存每个向量的坐标，而每个坐标都是一个浮点数，占用的内存就已经非常大了。

- 除此之外，还需要维护聚类中心和每个向量的聚类中心索引，这也会占用大量的内存。

对于第一个问题可以通过量化(Quantization)的方式解决，也就是常见的有损压缩。例如在内存中可以将聚类中心里面的每一个向量都用聚类中心的向量来表示，并维护一个所有向量到聚类中心的码本，这样就能大大减少内存的占用。

但这仍然不能解决所有问题，在前面一个例子中，在二维坐标系中划分了聚类中心，同理，在高维坐标系中，也可以划定多个聚类中心点，不断调整和迭代，直到找到多个稳定和收敛的中心点。

但是在高维坐标系中，还会遇到维度灾难问题，具体来说，随着维度的增加，数据点之间的距离会呈指数级增长，这也就意味着，在高维坐标系中，需要更多的聚类中心点将数据点分成更小的簇，才能提高分类的质量。否者，向量和自己的聚类中心距离很远，会极大的降低搜索的速度和质量。

但如果想要维持分类和搜索质量，就需要维护数量庞大的聚类中心。随之而来会带来另一个问题，那就是聚类中心点的数量会随着维度的增加而指数级增长，这样会导致我们存储码本的数量极速增加，从而极大的增加了内存的消耗。例如一个 128 维的向量，需要维护 2^64 个聚类中心才能维持不错的量化结果，但这样的码本存储大小已经超过维护原始向量的内存大小了。

解决这个问题的方法是将向量分解为多个子向量，然后对每个子向量独立进行量化，比如将 128 维的向量分为 8 个 16 维的向量，然后在 8 个 16 维的子向量上分别进行聚类，因为 16 维的子向量大概只需要 256 个聚类中心就能得到还不错的量化结果，所以就可以将码本的大小从 2^64 降低到 8 * 256 = 2048 个聚类中心，从而降低内存开销。

而将向量进行编码后，也将得到 8 个编码值，将它们拼起来就是该向量的最终编码值。等到使用的时候，只需要将这 8 个编码值，然后分别在 8 个子码本中搜索出对应的 16 维的向量，就能将它们使用笛卡尔积的方式组合成一个 128 维的向量，从而得到最终的搜索结果。这也就是乘积量化（Product Quantization）的原理。

使用 PQ 算法，可以显著的减少内存的开销，同时加快搜索的速度，它唯一的问题是搜索的质量会有所下降，但就像我们刚才所讲，所有算法都是在内存、速度和质量上做一个权衡。

## HNSW

除了聚类以外，也可以通过构建树或者构建图的方式来实现近似最近邻搜索。这种方法的基本思想是每次将向量加到数据库中的时候，就先找到与它最相邻的向量，然后将它们连接起来，这样就构成了一个图。当需要搜索的时候，就可以从图中的某个节点开始，不断的进行最相邻搜索和最短路径计算，直到找到最相似的向量。

这种算法能保证搜索的质量，但是如果图中所以的节点都以最短的路径相连，如图中最下面的一层，那么在搜索的时候，就同样需要遍历所有的节点。

可以用跳表的思想做优化

![](C:\Users\ljc\Documents\GitHub\--\数据库\向量数据库\图片\GD7ufK.jpg)

HNSW 继承了相同的分层格式，最高层具有更长的边缘（用于快速搜索），而较低层具有较短的边缘（用于准确搜索）。

具体来说，可以将图分为多层，每一层都是一个小世界，图中的节点都是相互连接的。而且每一层的节点都会连接到上一层的节点，当需要搜索的时候，就可以从第一层开始，因为第一层的节点之间距离很长，可以减少搜索的时间，然后再逐层向下搜索，又因为最下层相似节点之间相互关联，所以可以保证搜索的质量，能够找到最相似的向量。

HNSW 算法是一种经典的空间换时间的算法，它的搜索质量和搜索速度都比较高，但是它的内存开销也比较大，因为不仅需要将所有的向量都存储在内存中。还需要维护一个图的结构，也同样需要存储。所以这类算法需要根据实际的场景来选择。

## LSH（locality Sensitive Hashing）

局部敏感哈希

特点是快速，同时仍然提供一个近似、非穷举的结果。LSH使用一组哈希函数将相似向量映射到”桶“中。从而使相似向量具有相同的哈希值。这样，就可以通过比较哈希值来判断向量之间的相似度。

通常，我们设计的哈希算法都是力求减少哈希碰撞的次数，如果冲突就需要用链表等数据结构解决冲突。

但是在向量搜索中，我们的目的是为了找到相似的向量，所以专门设计一种哈希函数，使得碰撞概率尽可能高，并且位置越近或者越相似的向量越容易碰撞，这样相似的向量就会被映射到同一个桶中。

等搜索特定向量时，为了找到给定查询向量的最近邻居，使用相同的哈希函数将类似向量粪桶到哈希表中。查询向量被散列到特定表中，然后与该表中的其他向量进行比较已找到最接近的匹配项。

而这样的哈希函数可以通过随机设置几条边，然后看向量是否在边的一边。这个原理很简单，如果两个向量的距离很近，那么他们在直线的同一边的概率很高。

## Random Projection for LSH 随即投影

但是在高维空间中，数据点之间的距离往往非常稀疏，数据点之间的距离会随着维度的增加呈指数级增长。导致计算出来的桶非常多，最极端的就是一个桶一个向量。所以在实现LSH算法的时候，会考虑随机投影的方式，将高维空间的数据点投影到低维空间，从而减少计算的时间和提高查询的质量。

随即投影背后的基本思想就是使用随即投影矩阵，将高维向量投影到低维空间。创建一个有随机数构成的矩阵，其大小是所需的目标低维值。然后计算输入向量和矩阵之间的点积，得到一个被投影的矩阵，它比原始向量具有更少的维度但仍保留了它们之间的相似性。

当进行查询的时候，用相同的投影矩阵将查询向量投影到低维空间。然后进行比较，进而找到最近邻居。

## 相似性测量

欧氏距离，余弦相似度、点积相似度

欧氏距离相似度优点是可以反映响亮的绝对距离，适用于考虑向量长度的相似性计算。比如推荐系统中，根据用户历史行为推荐相似的商品，这是不仅仅要考虑用户历史行为的相似度，还要考虑历史行为的数量。

余弦相似度都对响亮的长度不敏感，只关注方向

点积相似度，简单快，对长度敏感，因此计算高维向量的相似性时可能会出现问题。

## 过滤

在实际场景中，往往不需要在整个向量数据库中进行相似性搜索，而是通过部分的业务字段进行过滤再进行查询。所以存储在数据库的向量往往还需要包含元数据，例如用户ID、文档ID等。这样就可以在搜索的时候，根据元数据来过滤搜索结果，从而得到最终的结果。

为此，向量数据库通常维护两个索引：一个是向量索引，另一个是元数据索引。然后，在进行相似性搜索本身之前或之后执行元数据的过滤，但无论哪种情况下，都存在导致查询过程变慢的困难。

Pre-filtering: 虽然减少了搜索空间，但也可能造成系统忽略与元数据筛选标准不匹配的相关结果。

Post-filtering：在向量搜索完成后进行元数据过滤。确保考虑所有相关结果，在搜索完成后将不相关的结果进行筛选

## 向量数据库选型

### 分布式

### 访问控制和备份

是否能快速的添加新的用户和权限控制，是否能够快速的添加新的节点，审计日志是否完善等等，都是需要考虑的因素。

另外，监控与备份也是一个重要因素，数显故障的时候可以快速定位问题和恢复数据。

### API & SDK

开发者最关心的

### 选型

| 向量数据库     | URL                                    | GitHub Star | Language      | Cloud |
| --------- | -------------------------------------- | ----------- | ------------- | ----- |
| chroma    | https://github.com/chroma-core/chroma  | 7.4K        | Python        | ❌     |
| milvus    | https://github.com/milvus-io/milvus    | 21.5K       | Go/Python/C++ | ✅     |
| pinecone  | https://www.pinecone.io/               | ❌           | ❌             | ✅     |
| qdrant    | https://github.com/qdrant/qdrant       | 11.8K       | Rust          | ✅     |
| typesense | https://github.com/typesense/typesense | 12.9K       | C++           | ❌     |
| weaviate  | https://github.com/weaviate/weaviate   | 6.9K        | Go            | ✅     |

### 传统数据的扩展

类似Redis除了传统的KV数据库用途外，Redis还提供了Redis Modules，这是一种通过新功能、命令和数据类型扩展Redis的方式。例如使用RedisSearch模块来扩展向量搜索的功能。

类似的还有PostgreSQL。
